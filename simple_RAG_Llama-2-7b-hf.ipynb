{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lib list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ ------------\n",
      "asttokens          2.4.1\n",
      "certifi            2024.2.2\n",
      "charset-normalizer 3.3.2\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "debugpy            1.8.1\n",
      "decorator          5.1.1\n",
      "executing          2.0.1\n",
      "filelock           3.13.1\n",
      "fsspec             2024.2.0\n",
      "huggingface-hub    0.22.2\n",
      "idna               3.7\n",
      "intel-openmp       2021.4.0\n",
      "ipykernel          6.29.4\n",
      "ipython            8.24.0\n",
      "ipywidgets         8.1.2\n",
      "jedi               0.19.1\n",
      "Jinja2             3.1.3\n",
      "joblib             1.4.0\n",
      "jupyter_client     8.6.1\n",
      "jupyter_core       5.7.2\n",
      "jupyterlab_widgets 3.0.10\n",
      "MarkupSafe         2.1.5\n",
      "matplotlib-inline  0.1.7\n",
      "mkl                2021.4.0\n",
      "mpmath             1.3.0\n",
      "nest-asyncio       1.6.0\n",
      "networkx           3.2.1\n",
      "numpy              1.26.3\n",
      "packaging          24.0\n",
      "parso              0.8.4\n",
      "pillow             10.2.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.1\n",
      "prompt-toolkit     3.0.43\n",
      "psutil             5.9.8\n",
      "pure-eval          0.2.2\n",
      "Pygments           2.17.2\n",
      "python-dateutil    2.9.0.post0\n",
      "pywin32            306\n",
      "PyYAML             6.0.1\n",
      "pyzmq              26.0.2\n",
      "regex              2024.4.16\n",
      "requests           2.31.0\n",
      "safetensors        0.4.3\n",
      "scikit-learn       1.4.2\n",
      "scipy              1.13.0\n",
      "six                1.16.0\n",
      "stack-data         0.6.3\n",
      "sympy              1.12\n",
      "tbb                2021.11.0\n",
      "threadpoolctl      3.4.0\n",
      "tokenizers         0.19.1\n",
      "torch              2.3.0+cu121\n",
      "torchaudio         2.3.0+cu121\n",
      "torchvision        0.18.0+cu121\n",
      "tornado            6.4\n",
      "tqdm               4.66.2\n",
      "traitlets          5.14.3\n",
      "transformers       4.40.1\n",
      "typing_extensions  4.9.0\n",
      "urllib3            2.2.1\n",
      "wcwidth            0.2.13\n",
      "widgetsnbextension 4.0.10\n"
     ]
    }
   ],
   "source": [
    "#libraries installed in venv\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_MODEL = 'NousResearch/Llama-2-7b-hf'\n",
    "DATA_SOURCE = 'ragnaros.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcs and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTFIDFRetriever:\n",
    "    def __init__(self, document_path):\n",
    "        with open(document_path, 'r', encoding='utf-8') as file:\n",
    "            self.documents = file.read().split('\\n\\n')\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)\n",
    "    \n",
    "    def retrieve(self, query, top_n=1):\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        indices = np.argsort(-similarities)[:top_n]\n",
    "        return [self.documents[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMAGenerator:\n",
    "    def __init__(self, model_name):\n",
    "        self.generator = pipeline('text-generation', model=model_name, device=0)\n",
    "\n",
    "    def generate(self, prompt, max_length=250):\n",
    "        return self.generator(prompt, max_length=max_length, num_return_sequences=1)[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb0e48f807144349c56deaf4e723f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbnfa\\Python_bgdn\\RAG\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bbnfa\\Python_bgdn\\RAG\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bbnfa\\Python_bgdn\\RAG\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bbnfa\\Python_bgdn\\RAG\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class RAG:\n",
    "    def __init__(self, retriever, generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "    \n",
    "    def answer(self, query):\n",
    "        contexts = self.retriever.retrieve(query)\n",
    "        answers = [self.generator.generate(context) for context in contexts]\n",
    "        return answers\n",
    "\n",
    "retriever = SimpleTFIDFRetriever(DATA_SOURCE)\n",
    "generator = LLaMAGenerator(SELECTED_MODEL)\n",
    "\n",
    "rag = RAG(retriever, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\bbnfa\\Python_bgdn\\RAG\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"Who defeated Ragnaros?\"\n",
    "answers = rag.answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Malfurion Stormrage rallied Hyjal's defenders in a massive counterattack, the Circle stormed into the Firelands\n",
      " Soon, the Circle was reinforced by the Alliance and Horde who pushed further into the Firelands until they reached Sulfuron Keep\n",
      "[18][19] During the battle against Ragnaros in the normal dungeon he retreats at 10% health saying that his attackers have come too soon\n",
      " However during the heroic fight when Ragnaros attempts to withdraw from the battle, he is interrupted by the arrival of Cenarius, Malfurion, and Hamuul, who each channel a powerful spell that solidifies the lava surrounding the Firelord, binding him\n",
      " With nowhere left to run Ragnaros bursts out of the lava and in a final battle with the adventurers, the fire lord is slain\n",
      " Due to Ragnaros dying in his plane, the Firelands, he also suffers the same fate as Al'Akir, that being eternal death\n",
      " He was the second Elemental Lord to die\n",
      "\n",
      "Ragnaros's death was a\n"
     ]
    }
   ],
   "source": [
    "print(answers[0].replace('.', '\\n'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
